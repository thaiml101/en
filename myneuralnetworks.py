# -*- coding: utf-8 -*-
"""myNeuralNetworks

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ALaM38gSYtzzSHBYb9n367YJLhG6doxA
"""

class myNeuralNetwork(object):
    
    def __init__(self, n_in, n_layer1, n_layer2, n_out, learning_rate):
        '''__init__
        Class constructor: Initialize the parameters of the network including
        the learning rate, layer sizes, and each of the parameters
        of the model (weights, placeholders for activations, inputs, 
        deltas for gradients, and weight gradients). This method
        should also initialize the weights of your model randomly
            Input:
                n_in:          number of inputs
                n_layer1:      number of nodes in layer 1
                n_layer2:      number of nodes in layer 2
                n_out:         number of output nodes
                learning_rate: learning rate for gradient descent
            Output:
                none
        '''
        self.n_in = n_in
        self.n_layer1 = n_layer1
        self.n_layer2 = n_layer2
        self.n_out = n_out
        self.learning_rate = learning_rate
        
        # initialize weights
        self.W1 = np.random.randn(n_in, n_layer1)
        self.W2 = np.random.randn(n_layer1, n_layer2)
        self.W3 = np.random.randn(n_layer2, n_out)
        
    def sigmoid(self, X):
        '''sigmoid
        Compute the sigmoid function for each value in matrix X
            Input:
                X: A matrix of any size [m x n]
            Output:
                X_sigmoid: A matrix [m x n] where each entry corresponds to the
                           entry of X after applying the sigmoid function
        '''
        return 1.0 / (1.0 + np.exp(-X))
    
    def sigmoid_derivative(self, X):
        '''sigmoid_derivative
        Compute the sigmoid derivative function for each value in matrix X
            Input:
                X: A matrix of any size [m x n]
            Output:
                X_sigmoid: A matrix [m x n] where each entry corresponds to the
                           entry of X after applying the sigmoid derivative function
        '''
        return X * (1 - X)
    
    def forward_propagation(self, x):
        '''forward_propagation
        Takes a vector of your input data (one sample) and feeds
        it forward through the neural network, calculating activations and
        layer node values along the way.
            Input:
                x: a vector of data representing 1 sample [n_in x 1]
            Output:
                y_hat: a vector (or scaler of predictions) [n_out x 1]
                (typically n_out will be 1 for binary classification)
        '''
        # input layer -> hidden layer 1
        self.a1 = self.sigmoid(np.dot(x, self.W1))
        # hidden layer 1 -> hidden layer 2
        self.a2 = self.sigmoid(np.dot(self.a1, self.W2))
        # hidden layer 2 -> output layer
        y_hat = self.sigmoid(np.dot(self.a2, self.W3))
        return y_hat
    
    def compute_loss(self, X, y):
        '''compute_loss
        Computes the current loss/cost function of the neural network
        based on the weights and the data input into this function.
        To do so, it runs the X data through the network to generate
        predictions, then compares it to the target variable y using
        the cost/loss function
            Input:
                X: A matrix of N samples of data [N x n_in]
                y: Target variable [N x 1]
            Output:
                loss: a scalar measure of loss/cost
        '''
        y_hat = self.forward_propagation(X)
        return np.mean(0.5 * (y - y_hat)**2)
    
    def backpropagate(self, x, y):
        '''backpropagate
        Backpropagate the error from one sample determining the gradients
        with respect to each of the weights in the network. The steps for
        this algorithm are:
            1. Run a forward pass of the model to get the activations 
               Corresponding to x and get the loss functionof the model 
               predictions compared to the target variable y
            2. Compute the deltas (see lecture notes) and values of the
               gradient with respect to each weight in each layer moving
               backwards through the network
    
            Input:
                x: A vector of 1 samples of data [n_in x 1]
                y: Target variable [scalar]
            Output:
                loss: a scalar measure of th loss/cost associated with x,y
                      and the current model weights
        '''
        y_hat = self.forward_propagation(x)
        # output layer -> hidden layer 2
        delta3 = np.multiply(-(y - y_hat), self.sigmoid_derivative(y_hat))
        dW3 = np.dot(self.a2.T, delta3)
        # hidden layer 2 -> hidden layer 1
        delta2 = np.dot(delta3, self.W3.T) * self.sigmoid_derivative(self.a2)
        dW2 = np.dot(self.a1.T, delta2)
        # hidden layer 1 -> input layer
        delta1 = np.dot(delta2, self.W2.T) * self.sigmoid_derivative(self.a1)
        dW1 = np.dot(x.T, delta1)
        
        # update weights
        self.W1 -= self.learning_rate * dW1
        self.W2 -= self.learning_rate * dW2
        self.W3 -= self.learning_rate * dW3
        
        return self.compute_loss(x, y)
    
    def stochastic_gradient_descent_step(self, X, y):
        '''stochastic_gradient_descent_step [OPTIONAL - you may also do this
        directly in backpropagate]
        Using the gradient values computed by backpropagate, update each
        weight value of the model according to the familiar stochastic
        gradient descent update equation.
        
        Input: none
        Output: none
        '''
        # randomly shuffle training data
        idx = np.random.permutation(X.shape[0])
        for i in idx:
            self.backpropagate(X[i], y[i])
            
    def fit(self, X, y, max_epochs=100, learning_rate=None, get_validation_loss=False):
        '''fit
            Input:
                X: A matrix of N samples of data [N x n_in]
                y: Target variable [N x 1]
            Output:
                training_loss:   Vector of training loss values at the end of each epoch
                validation_loss: Vector of validation loss values at the end of each epoch
                                 [optional output if get_validation_loss==True]
        '''
        if learning_rate is not None:
            self.learning_rate = learning_rate
            
        training_loss = []
        validation_loss = []
        for epoch in range(max_epochs):
            self.stochastic_gradient_descent_step(X, y)
            training_loss.append(self.compute_loss(X, y))
            if get_validation_loss:
                validation_loss.append(self.compute_loss(X_val, y_val))
                
        if get_validation_loss:
            return training_loss, validation_loss
        else:
            return training_loss
    
    def predict_proba(self, X):
        '''predict_proba
        Compute the output of the neural network for each sample in X, with the last layer's
        sigmoid activation providing an estimate of the target output between 0 and 1
            Input:
                X: A matrix of N samples of data [N x n_in]
            Output:
                y_hat: A vector of class predictions between 0 and 1 [N x 1]
        '''
        return self.forward_propagation(X)
    
    def predict(self, X, decision_thresh=0.5):
        '''predict
        Compute the output of the neural network prediction for 
        each sample in X, with the last layer's sigmoid activation 
        providing an estimate of the target output between 0 and 1, 
        then thresholding that prediction based on decision_thresh
        to produce a binary class prediction
            Input:
                X: A matrix of N samples of data [N x n_in]
                decision_threshold: threshold for the class confidence score
                                    of predict_proba for binarizing the output
            Output:
                y_hat: A vector of class predictions of either 0 or 1 [N x 1]
        '''
        return (self.forward_propagation(X) > decision_thresh).astype(int)